---
layout: post
title: About Agent
tags: agent
math: true
date: 2025-11-06 19:26 +0800
---

# Agent概念
**摘要**
本文是一个完整的关于人工智能智能体(AI Agent)的介绍。随着大语言模型(Large Language Model,LLM)的爆发，其在工业界各个领域也在逐步地落地，并逐渐深入到我们个人的工作与生活中。其中AI Agent这一基于大语言模型的应用成为2025年AI领域最火热的技术名词。本文从AI Agent的基础概念开始，基于学术界的论文给出AI Agent一个准确直白的定义，并详细介绍了Agent的发展历史：从智能体思想的诞生到智能体应用到我们的日常。接着从Agent的三大核心：规划、记忆和实用工具详细剖析了一个成熟的智能体所包含的组成部分，并简要概述了多智能体的协同。本文的第二部分介绍了四个智能体开发平台：Dify、n8n、LangChain和OpenManus。并在本地部署进行了简单的体验，最终给出四者的比较。最后在实际的应用场景-网络流量分析下，笔者使用OpenManus这一开源框架，调用一个基于pyshark编写的MCP Server，实现了简单的网络流量分析功能。
## 一、智能体概述
### 1.1 智能体的定义
智能体（Agent）是一种能够感知环境、制定决策并采取行动以实现特定目标的AI系统，一般具有记忆、规划、使用工具、采取行为等基本能力。
在论文[Multi-Agent Collaboration Mechanisms: A Survey of LLMs](https://arxiv.org/pdf/2501.06322)中给智能体一个数学表述：$a=\{ m,o,e,x,y \}$
- ==Model *m*== $=\{arch,mem,adp\}$
- ==Objective *o*==: Agent的目标或目的，指导其在系统内的行动
- ==Environment *e*==:代理运行的状态或条件所构成的环境或上下文.在大语言模型中，上下文窗口通常受限于token数量。
- ==Input *x*==
- ==Output *y*==:相应的行动或输出，由函数 $y=m(o,e,x)$定义，代理使用其模型、上下文和目标对输入$x$进行操作。

得益于大语言模型的突出性能，目前的智能体大多是由大语言模型作为驱动。在大语言模型驱动的智能体中，大语言模型作为Agent的大脑，主要完成规划、记忆和使用工具三个部分。
+ 规划
	+ 子目标和分解：智能体将大型任务分解为较小的、可管理的子目标，从而能够高效地处理复杂任务。
	+ 反思和改进：智能体可以对过去的行动进行自我批评和自我反思，从错误中学习，并对未来步骤进行改进，从而提高最终结果的质量。
- 记忆
	- 短期记忆：将所有上下文学习（提示工程）视为利用模型的短期记忆进行学习。
	- 这使智能体能够利用外部向量存储和快速检索，在较长时间内保留和回忆（无限）信息。
- 使用工具
	- 智能体学会调用外部API以获取模型权重中缺失的额外信息（通常在预训练后难以更改），包括最新信息、代码执行能力、访问专有信息源等。目前主要使用模型上下文协议（Model Context Protocol，MCP）实现。
### 1.2 Agent发展历史

>[!Note]
>来源于复旦大学Agent综述《The Rise and Potential of Large Language Model Based Agents: A Survey》
>以及博客：[【愚公系列】《AI Agent技术、应用与商业》002-Al Agent的发展历程-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/2503906)

#### 1.2.1 概念发展历史
AI Agent 作为一种技术概念，伴随着人工智能（AI）技术不断演进和应用场景不断拓展而发展。其发展历程经历了从哲学启蒙到工程实践的漫长过程，从最初的理论探索到如今在各行各业的广泛应用。这个过程不仅体现了技术的成熟，也展现了人类在智能体构建方面思想的逐步深入。

很多人可能会认为，AI Agent 是大语言模型（LLM）的产物，尤其是当下基于 GPT-4 的 AutoGPT、BabyGPT、MetaGPT 等开源 Agent 项目风头正劲。然而，AI Agent 的概念并不是今天才有的，它自人工智能概念诞生之初便逐渐演化，成为今日我们所知的形态。

在论文[2309.07864](https://arxiv.org/pdf/2309.07864)中，作者认为这个哲学思想早已存在。人工智能是一个致力于设计和开发能够复制人类智能和能力的系统的领域。早在18世纪，哲学家**丹尼斯·狄德罗**就提出了一个观点，即如果一只鹦鹉能够回答每一个问题，那么它就可以被视为智能的。尽管狄德罗指的是像鹦鹉这样的生物，但他的观点突显了一个深刻的概念，即一个高度智能的生物可能类似于人类智能。

在20世纪50年代，**艾伦·图灵**将这一观点扩展到人工实体，并提出了著名的图灵测试。这一测试是AI领域的基石，旨在探索机器是否能够展现出与人类相当的智能行为。这些AI实体通常被称为“代理”，构成了AI系统的基石。 智能体的概念起源于哲学，其根源可以追溯到亚里士多德和休谟等思想家。它描述了具有欲望、信念、意图以及采取行动能力的实体。这一观点随后过渡到计算机科学领域，旨在使计算机能够理解用户的兴趣，并自主地代表用户执行操作。

在1995年，Wooldridge和Jennings提出了AI Agent的正式定义。他们认为，AI Agent是一个能够在某个环境中自主行动、以实现设计目标的计算机系统。他们进一步提出，AI Agent应具备四个基本属性：自主性、反应性、社会能力和主动性。随着这个定义的确立，AI Agent不仅被应用于一些复杂的任务中，还能够通过感知环境并采取行动来提高成功的机会。事实上，像下棋机器人（例如国际象棋程序）这样的简单程序也可以被视作AI Agent。

2020年，**OpenAI**公司发布GPT3，这是一个参数量达到1750亿的大模型。革命性地提升了对于自然语言的理解能力。随后一系列地大语言模型雨后春笋般涌现。大语言模型的爆火为AI Agent的多元化应用提供了新的契机。2023年3月，OpenAI发布GPT4，同月推出AutoGPT。AutoGPT能够在无需用户反复提问的情况下，自动完成任务。它可以通过API读取文件、浏览网页、审查提示历史等，至此，基于大语言模型的智能体出现，标志着AI Agent迈向自我驱动的新时代。
#### 1.2.2 技术演变历史
AI Agent的发展离不开AI技术的支撑，不同历史阶段的AI Agent形态差异，源于当时相关技术的突破与应用。因此，了解AI Agent技术的演变史，有助于我们更好地理解其发展与趋势。复旦大学NLP团队在其论文《The Rise and Potential of Large Language Model Based Agents: A Survey》中，将AI Agent的技术演变划分为以下五个阶段：
##### 阶段一：符号Agent
在AI研究的早期阶段，符号AI占据主导地位。符号AI的特点是依赖于符号逻辑，其方法使用逻辑规则和符号表示来封装知识并促进推理过程。早期的AI Agent便是基于这种方式构建的，主要关注两个核心问题：

1. 转导问题：如何从已有的知识推导出新的信息。
2. 表示/推理问题：如何高效地表示知识并进行推理。

符号Agent旨在模仿人类思维，具备明确且可解释的推理框架。由于符号的高表达能力，符号Agent能够在处理知识系统时展现出极强的表达能力。一个典型例子就是基于知识的专家系统。

然而，符号Agent在面对不确定性和大规模实际问题时存在明显的限制。例如，由于符号推理算法的复杂性，设计出能够在有限时间内有效得出结论的高效算法成为一个巨大的挑战。这些问题也成为AI Agent技术演变的重要驱动力。
##### 阶段二：反应型Agent
反应型Agent与符号Agent有显著不同，它不依赖于复杂的符号推理，而是专注于与环境的交互，强调快速响应和实时反应。其主要基于感知-行动循环（Perception-Action Loop）：

- 感知：Agent感知环境变化。
- 行动：根据感知结果采取适当的行动。

反应型Agent的设计优先考虑输入/输出映射，而不是复杂的推理和符号操作。它通常需要较少的计算资源，能够实现较快的响应。虽然这种方式在很多应用中非常高效，但反应型Agent也有一定局限性，比如它们可能缺乏高级决策能力和规划能力。
##### 阶段三：基于强化学习的Agent
随着计算能力的提升和数据可用性的增加，研究人员开始尝试利用强化学习（Reinforcement Learning）来训练Agent，以应对更复杂的任务。强化学习的核心在于通过与环境的互动来学习，使Agent在特定任务中获得最大化的累积奖励。

最初，强化学习Agent主要依赖策略搜索和价值函数优化等基本技术。随着深度学习的兴起，深度强化学习（Deep Reinforcement Learning）得到了发展。这一方法将深度神经网络与强化学习相结合，使得Agent能够从高维输入中学习复杂的策略，取得了如AlphaGo和DQN（深度Q网络）等重大突破。

深度强化学习的优势在于，Agent可以在未知环境中自主学习，无需人工干预，且广泛应用于游戏、机器人控制等领域。但它也面临着一些挑战，如训练时间长、样本效率低、稳定性差等问题，尤其在复杂的现实环境中。
##### 阶段四：具有迁移学习与元学习的Agent
传统的强化学习方法需要大量的训练样本和时间，而且在面对新任务时，Agent缺乏泛化能力。为此，研究人员引入了迁移学习（Transfer Learning）和元学习（Meta Learning）技术，来提高学习效率和提升性能。

1. 迁移学习：通过在已有任务中获得的知识帮助Agent快速学习新任务，从而加速学习过程。这种方法在不同任务之间共享和迁移知识，能有效提高泛化能力。
2. 元学习：元学习的核心是“学习如何学习”，使Agent能够从少量样本中迅速推断出最优策略，并在面对新任务时调整学习方法，减少对大量样本的依赖。元学习特别适用于那些源任务和目标任务之间存在较大差异的情况，但它也面临着预训练和大量样本需求等挑战。
##### 阶段五：基于大语言模型的Agent
随着大规模语言模型（LLM）如GPT等的问世，并展示出卓越的自然语言处理能力，研究人员开始将LLM作为AI Agent的核心构建模块。LLM作为AI Agent的大脑，能够通过多模态感知和工具使用等手段，扩展其感知和行动能力。

基于LLM的A IAgent具有以下特征：

1. 推理与规划能力：通过技术如Chain-of-Thought（CoT）和问题分解，它能够展示出类似符号Agent的推理和规划能力。
2. 环境交互：基于LLM的AI Agent可以从与环境的互动中学习和执行新的行动，类似于反应型Agent的交互能力。
3. 无缝转移与少样本学习：LLM经过大规模语料库的预训练，展示了强大的少样本学习和零样本泛化能力，使其能够在任务之间无缝转移，无需更新参数。
4. 实际应用：基于LLM的AI Agent已经广泛应用于软件开发、科学研究等多个领域。自然语言理解和生成能力使它们能够与用户无缝交互，并通过多个Agent之间的协作与竞争来提升效率和创新。

研究还表明，多个基于LLM的Agent共存时，可以引发类似社会现象的行为模式，展现出多智能体系统的潜力。


### 1.3 智能体核心1--Planning
无法进行规划，只能进行语言交互的智能体不过是里聊天机器人。能够自主思考的智能体接受任务后，可以制定解决方案然后返回结果。一个复杂的任务包含很多步骤。智能体需要知道这些步骤，并在执行前进行规划。
#### 1.3.1 任务分解
关于任务分解最经典的方法是**链式思维Chain of thought（CoT）** https://arxiv.org/abs/2201.11903 ，该方法指示模型“逐步思考”，以便在测试时利用更多的计算能力，将困难的任务分解为更小、更简单的步骤。CoT将大型任务转化为多个可管理的任务，并揭示了模型思维过程的解释。

思维树Tree of Thoughts( https://arxiv.org/abs/2305.10601 )在通过在每个步骤中探索多种推理可能性扩展了CoT。它首先将问题分解为多个思维步骤，并在每个步骤中生成多个想法，形成树状结构。搜索过程可以是广度优先搜索（BFS）或深度优先搜索（DFS），每个状态通过分类器（通过提示）或多数投票进行评估。任务分解可以通过以下方式完成：
- （1）使用简单的提示（如“XYZ的步骤”，“实现XYZ的子目标是什么？”）由大语言模型完成；
- （2）使用特定任务的指令，例如“编写小说大纲”；
- （3）通过人工输入

另一种截然不同的方法是LLM+P（ https://arxiv.org/abs/2304.11477 ），该方法依赖外部经典规划器进行长期规划。此方法利用规划领域定义语言（PDDL）作为描述规划问题的中间接口。在此过程中，大语言模型（1）将问题翻译为“问题PDDL”，然后（2）请求经典规划器根据现有的“领域PDDL”生成PDDL计划，最后（3）将PDDL计划重新翻译为自然语言。本质上，规划步骤被外包给外部工具，假设存在特定领域的PDDL和合适的规划器，这在某些机器人设置中很常见，但在许多其他领域中并不常见。

#### 1.3.2 自我反思

自我反思是自主智能体迭代改进的一个重要方面，它通过完善过去的行动决策和纠正之前的错误来实现。在现实世界的任务中，试错是不可避免的，而自我反思在其中发挥着关键作用。

ReAct范式(https://arxiv.org/pdf/2210.03629 )即Synergizing REasoning +ACTing通过扩展行动空间，将任务特定的离散行动和语言空间结合起来，在大语言模型中整合了推理和行动。前者使大语言模型能够与环境互动（例如使用维基百科搜索API），而后者则促使大语言模型以自然语言生成推理轨迹。

ReAct 的灵感来自于 “行为” 和 “推理” 之间的协同作用，正是这种协同作用使得人类能够学习新任务并做出决策或推理。

链式思考 (CoT) 提示显示了 LLMs 执行推理轨迹以生成涉及算术和常识推理的问题的答案的能力，以及其他任务 [(Wei 等人，2022)(opens in a new tab)](https://arxiv.org/abs/2201.11903)。但它因缺乏和外部世界的接触或无法更新自己的知识，而导致事实幻觉和错误传播等问题。

ReAct 是一个将==推理和行为==与 LLMs 相结合通用的范例。ReAct 提示 LLMs 为任务生成口头推理轨迹和操作。这使得系统执行动态推理来创建、维护和调整操作计划，同时还支持与外部环境(例如，Wikipedia)的交互，以将额外信息合并到推理中，从而克服问答中的幻觉。提高了AI Agent决策过程的透明度，因为可以检查推理链以进行调试或评估可信度。

ReAct提示模板为大语言模型的思考设定了明确的步骤，大致格式为：
```plaintext
思考: ...
行动: ...
观察: ...
... (重复很多次)
```

这个思考行动观察循环重复进行，允许AI智能体在需要时链接多个工具使用。事实证明，在知识密集型任务和决策任务的实验中，ReAct的表现优于没有“思考: ...”步骤的基线。

Reflexion（ https://arxiv.org/pdf/2303.11366 ）是一个框架，旨在为智能体配备动态记忆和自我反思能力，以提升其推理技能。Reflexion采用标准的强化学习设置，其中奖励模型提供简单的二元奖励，而行动空间遵循ReAct的设置，即任务特定的行动空间通过语言增强，以支持复杂的推理步骤。在每次行动$a_t$之后，智能体会计算一个==启发式函数$h_t$==，并且可以根据自我反思的结果选择性地决定重置环境以开始新的尝试。

启发式函数用于判断轨迹是否低效或包含幻觉，并决定是否停止。低效规划是指在没有成功的情况下持续时间过长的轨迹。幻觉被定义为遇到一系列连续相同的动作，这些动作在环境中导致相同的观察结果。

自我反思是通过向大语言模型展示两个示例来实现的，每个示例都是一对（失败的轨迹，用于指导未来计划变更的理想反思）。然后，反思被添加到智能体的工作记忆中，最多三个，作为查询大语言模型的上下文。

### 1.4 智能体核心2--记忆
记忆可以被定义为用于获取、存储、保留以及稍后检索信息的过程。首先简单介绍一下人类大脑中有几种类型的记忆。

==感觉记忆：==这是记忆的最初阶段，提供了在原始刺激结束后保留感觉信息（视觉、听觉等）的印象的能力。感觉记忆通常只持续几秒钟。其子类别包括图像记忆（视觉）、回声记忆（听觉）和触觉记忆（触觉）。

==短时记忆（STM）或工作记忆：==它存储我们当前意识到的信息，并用于执行复杂认知任务，如学习和推理。短时记忆被认为可以容纳大约7个项目（米勒，1956年），持续时间为20-30秒。

==长时记忆（LTM）：==长时记忆可以存储信息的时间非常长，从几天到几十年不等，其存储容量几乎是无限的。长时记忆有两种亚型：

==显性/陈述性记忆：==这是对事实和事件的记忆，指的是那些可以有意识回忆的记忆，包括情景记忆（事件和经历）和语义记忆（事实和概念）。 隐性/程序性记忆：这种记忆是无意识的，涉及自动执行的技能和程序，比如骑自行车或在键盘上打字。
由于人工智能很大程度上是对人脑的解释与模拟，因此可以简单地做如下映射：
- 感觉记忆对应于为原始输入（包括文本、图像或其他模态）学习嵌入表示；
- 短时记忆对应于上下文中的学习。它很短暂且有限，因为它受到Transformer有限上下文窗口长度的限制。
- 长时记忆对应于代理在查询时可以关注的外部向量存储，可以通过快速检索访问。

大语言模型在处理和生成文本方面表现出色，但它们在本质上是==无状态（stateless）==的。类比到人类大脑也就是仅处在感觉记忆阶段。这意味着每次与LLM的交互都是独立的，模型本身不会“记住”过去的对话或经验。大模型在“记忆”上主要局限于：

- **上下文窗口的限制导致遗忘问题。** LLM通过一个有限的“上下文窗口”（Context Window）来处理信息。所有输入（包括Prompt和之前的对话片段）都必须塞入这个窗口。一旦信息超出这个窗口，LLM就“忘记”了它，无法再访问。这导致了所谓的“遗忘”问题；
- **难以处理多轮/复杂任务。** 对于需要跨越多轮对话、追踪状态或执行一系列子任务的复杂任务，LLM很难保持连贯性和进展，因为它会不断“忘记”之前的步骤和决策。特别是在Agent的场景，工具的定义和工具的返回值都会存在于上下文中，同时由于Agent具有自主工作的能力，和LLM的平均交互的轮数也大大增加；
- **无法个性化。** 由于不记住特定用户的历史偏好、习惯或之前的互动，LLM难以提供真正个性化的体验。每次互动都像是第一次见面；
- **长上下文带来的性能和成本影响。** 推理速度变慢：LLM在处理更长的上下文时，需要进行更多的计算来处理和理解所有输入信息。这会导致推理时间增加，响应速度变慢。模型在长上下文下的性能可能下降：尽管LLM的上下文窗口越来越大，但研究发现，模型在超长上下文中检索关键信息的能力可能会下降。更高的token成本：上下文越长，输入的token数量就越多，从而导致每次API调用的成本更高。对于需要频繁交互或处理大量文本的应用来说，这会迅速累积成可观的费用。

智能体的记忆系统主要分为短期记忆和长期记忆两大类。

短期记忆（Short-term Memory, STM）是智能体维护当前对话和任务的即时上下文系统，主要包括：
- **会话缓冲（Context）记忆** ：保留最近对话历史的滚动窗口，确保回答上下文相关性；
- **工作记忆**：存储当前任务的临时信息，如中间结果、变量值等。
短期记忆受限于上下文窗口大小，适用于简单对话和单一任务场景。

长期记忆（Long-term Memory, LTM）是智能体用于跨会话、跨任务长期保存知识的记忆形式。它对应于人类的大脑中持久保存的记忆，例如事实知识、过去经历等。长期记忆的实现通常依赖于外部存储或知识库，外部存储可以缓解注意力持续时间有限的限制。常见的做法是将信息的嵌入表示存储到支持快速最大内积搜索（MIPS）的向量数据库中以加速检索。为了优化检索速度，通常选择近似最近邻（ANN）算法，返回近似的前k个最近邻，以牺牲少量精度换取显著的速度提升。
长期记忆使智能体能够随时间累积经验和知识，它特别适用于知识密集型应用和需要长期个性化的场景。

### 1.5 智能体核心3--使用工具
使用是人类一项卓越而独特的技能。我们通过创造、改造和利用外部物体，能够突破自身生理与认知的局限。为大型语言模型配备外部工具，可显著扩展其能力边界。
##### A.Function calling
Function calling是一种强大的能力，使大语言模型能够以结构化的方式与代码和外部系统进行交互。大语言模型不仅仅生成文本回应，还能理解何时调用特定的函数，并提供执行实际操作所需的参数。( https://huggingface.co/docs/hugs/en/guides/function-calling )


这里的function就是工具，每个function必须具有以下三个部分：
- 一个独特的名称
- 一个清晰的描述
- 一个定义预期参数的 JSON 模式
##### B.MCP
MCP 通过提供一种**标准化、安全且高效**的协议，解决了AI模型与外部工具和数据源交互时的诸多挑战。它简化了开发流程，促进了工具生态的繁荣，并使得AI应用（尤其是AI Agent）能够更灵活、更强大地执行复杂任务。
我们可以将MCP想象为AI应用程序的USB-C接口，就像 USB-C 为设备连接各种外设和配件提供了标准化的方式一样，MCP 为 AI 模型连接各种数据源和工具提供了标准化的接口。

MCP核心采用客户端-服务器(client-server)架构，主机引用可以连接多个服务器：

**MCP局限：**
1. 工具文档至关重要，LLM通过工具的文本描述来理解和选择工具，因此需要精心编写工具名称、docstring和参数说明。
2. 工具的使用和理解严重依赖于LLM基础模型能力，工具的数量严重依赖于LLM的长上下文的记忆能力
3. 每一次模型调用只能选择有限的工具(人类可以创造工具)，不能做到真正的智能，随着大模型基础能力的提升可能被逐步替换。
###### MCP的通信方式
MCP 使用 JSON-RPC 对消息进行编码。JSON-RPC 消息必须使用 UTF-8 编码。 目前，协议定义了两种标准的客户端与服务器通信传输机制：
- stdio，通过标准输入和标准输出进行通信
- 可流式传输的 HTTP 客户端应在可能的情况下支持 stdio。 客户端和服务器还可以以可插拔的方式实现自定义传输机制。==注意这种方式替代了之前的SSE(2024-11-05)==

在 stdio 传输中：
- 客户端将 MCP 服务器作为子进程启动。
- 服务器从其标准输入（stdin）读取 JSON-RPC 消息，并通过其标准输出（stdout）发送消息。
- 消息是单独的 JSON-RPC 请求、通知或响应。
- 消息通过换行符分隔，并且不得包含嵌入的换行符。
- 服务器可以将 UTF-8 字符串写入其标准错误（stderr）以用于日志记录。客户端可以选择捕获、转发或忽略这些日志。
- 服务器不得在其 stdout 中写入任何不是有效 MCP 消息的内容。
- 客户端不得向服务器的 stdin 写入任何不是有效 MCP 消息的内容

在可流式传输的 HTTP 传输中，服务器作为一个独立的进程运行，能够处理多个客户端连接。这种传输使用 HTTP POST 和 GET 请求。服务器可以选择使用服务器发送事件（SSE）来流式传输多个服务器消息。这允许基本的 MCP 服务器以及支持流式传输和服务器到客户端通知及请求的更功能丰富的服务器运行。服务器必须提供一个单一的 HTTP 端点路径。
#### Function calling与MCP
>[!Note]
>注意！Function calling和MCP不是同一个东西，两者实际处于大模型调用工具的不同步骤。不能认为MCP替代了Function calling

[MCP 与 Function Calling 到底什么关系，以及为什么我认为大部分人的观点都是错误的_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV15YJTzkENC/?spm_id_from=333.337.search-card.all.click&vd_source=65f808be57f694f1041e534ce34f3428)

上面这个视频的博主制作了一张图：

可以看到Function Calling是模型(模型API)的能力,而MCP所做的事情在服务器的层面，并非在模型层面。
### 1.6 多智能体协同
让一个LLM做了太多事情，尤其是在意图识别时会产生很多偏差。采用多智能体的协同合作，可以缓解整个问题。
多智能体系统(MASs)借鉴了人类社会的协作原则，通过分工合作，结合各个智能体的优势，共同完成任务。
